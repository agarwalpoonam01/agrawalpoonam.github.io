<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="https://www.w3.org/2005/Atom">
  <channel>
    <title>Life  is  Short  Do  More</title>
    <description>This is the jekyllTheme example blog. It only exists to demonstrate how the theme looks and feels.</description>
    <link>/</link>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml"/>
      <pubDate>Sat, 03 Feb 2024 11:07:18 +0530</pubDate>
    <lastBuildDate>Sat, 03 Feb 2024 11:07:18 +0530</lastBuildDate>
    <generator>Jekyll v3.9.0</generator>
    
      <item>
        <title>Distributed Tracing</title>
        <description>&lt;h3 id=&quot;distributed-tracing&quot;&gt;Distributed tracing:&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;- It is hard to locate the issue when something goes wrong in distributed microservice system as the services are distributed across multiple servers in different location, so traditional monitoring method like logs and metrics are no longer enough to understand where the fault is.
- Distributed tracing enables visibility of how request flows into the system by collecting the information such as time taken at each step , errors, failures etc.
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;components-of-distributed-tracing&quot;&gt;Components of Distributed tracing:&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;- Trace: Request from source to destination having different service hops in between. Traces are made up of spans.
- Span: A Single operation within a service. A service can have multiple spans. Primary building block of a trace. 
- Context: Passing context between different components or service in a distributed system.
- Instrumentation: Instrumentation library is component that developers integrate into their applications to generate, collect and manage trace data.
- Data Store: Typically storage system that stores trace data. Eg. Elasticsearch, Cassandra, Jaeger
- Visualization: Graphical representation of the trace data. Eg. Kibana, Jaeger UI
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;how-does-distributed-tracing-work&quot;&gt;How does distributed tracing work?&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;- Trace-id gets attached to an incoming request to the system. This identifies the request during its lifespan within the system.
- For every Child Span their is a Parent Span id associated.
- While the request flows in the system instrumentation library keeps on adding context data such as start time, end time, and metadata about each operation (span). 
- Trace data gets stored and then is visualized.
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;figure&gt;
&lt;img src=&quot;/media/img/distributed-tracing.png&quot; /&gt;
&lt;/figure&gt;

&lt;/div&gt;

</description>
        <pubDate>Sat, 30 Dec 2023 15:20:00 +0530</pubDate>
        <link>/blog/tutorial/Distributed-Tracing</link>
        <guid isPermaLink="true">/blog/tutorial/Distributed-Tracing</guid>
        
        <category>Basics</category>
        
        
        <category>Tutorial</category>
        
      </item>
    
      <item>
        <title>Elasticsearch Components</title>
        <description>&lt;h3 id=&quot;what-are-the-components-of-elk-stack&quot;&gt;What are the components of ELK Stack?&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;- E=? Elasticsearch. 
- It is distributed store, search and analytics engine based on Apache lucene. Elasticsearch is where indexing, search and analytics happens. 

- L=? Logstash.
- Collects, aggregates, enrich and ships data to elasticsearch.

- K=? Kibana.
- With Kibana one can explore, visualize the data in elasticsearch.

- There are few more components that can send data to elasticsearch - Beats, APM, external services.

- There are different kind of beats shipper, for eg. filebeat, metricbeat, packetbeat, winlogbeat etc.
- Beats=? For eg. Filebeat can collect data from files in distributed serevrs and send data to the logstash or directly to elasticsearch.

- APM=? Application Performane Monitoring. Capture and analyze distributed transactions in distributed system and sends the captured trace data to elasticsearch.

- External services=? can send data directly to elasticsearch using elasticsearch api end points.
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;figure&gt;
&lt;img src=&quot;/media/img/elasticsearch-components.png&quot; /&gt;
&lt;/figure&gt;

&lt;/div&gt;

</description>
        <pubDate>Fri, 15 Dec 2023 04:00:00 +0530</pubDate>
        <link>/blog/tutorial/Elasticsearch-Components</link>
        <guid isPermaLink="true">/blog/tutorial/Elasticsearch-Components</guid>
        
        <category>Basics</category>
        
        
        <category>Tutorial</category>
        
      </item>
    
      <item>
        <title>Elasticsearch Architecture</title>
        <description>&lt;h3 id=&quot;elasticsearch-architecture&quot;&gt;Elasticsearch Architecture&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;- An elasticsearch cluster is made up of number of nodes.
- Each node is nothing but an instance of elasticsearch.
- The data in elasticsearch is stored in index, you can compare it as database in relational database.
- The index can have any limit of data but this can create problem while operation as the index is large.
- Now, to deal with this situation a shard needs to be created as a unit of data that represents a subset of a larger index. There can be more than one shard for an index.
- Shard contains data represented as documents. 
- Documents can be compared with rows in relational database.
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;figure&gt;
&lt;img src=&quot;/media/img/elasticsearch-architecture.png&quot; /&gt;
&lt;/figure&gt;

&lt;/div&gt;

</description>
        <pubDate>Sun, 03 Dec 2023 15:20:00 +0530</pubDate>
        <link>/blog/tutorial/Elasticsearch-Architecture</link>
        <guid isPermaLink="true">/blog/tutorial/Elasticsearch-Architecture</guid>
        
        <category>Basics</category>
        
        
        <category>Tutorial</category>
        
      </item>
    
      <item>
        <title>Elasticsearch at a glance</title>
        <description>&lt;h3 id=&quot;what&quot;&gt;What?&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt; - ElasticSearch is a search engine (lucene based )
 - Which stores JSON data (using the inverse index data structure) 
 - Processes it
 - Supports full text based search using REST APIs
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;why&quot;&gt;Why?&lt;/h3&gt;

&lt;div&gt;
&lt;figure&gt;
&lt;img src=&quot;/media/img/why-elastic.png&quot; /&gt;
&lt;/figure&gt;

&lt;/div&gt;

&lt;h3 id=&quot;how&quot;&gt;How?&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt; - A document is like a row in a relational database, representing a given entity — the thing you’re searching for. 
 For example, a document can represent an encyclopedia article or log entries from a web server.   
 - An index is a collection of documents that have similar characteristics.
 - An index in Elasticsearch is actually what’s called an inverted index.
 - Instead of searching the text directly it searches the index.
 - An inverted index lists every unique word that appears in any text being searched and identifies all of the saved document for each word occur. 
 - The term “name” occurs in document A and B, so it is mapped to that document.
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;

&lt;figure&gt;
&lt;img src=&quot;/media/img/inverted-index.png&quot; /&gt;
&lt;/figure&gt;

&lt;/div&gt;
</description>
        <pubDate>Thu, 09 Nov 2023 15:20:00 +0530</pubDate>
        <link>/blog/tutorial/Elasticsearch</link>
        <guid isPermaLink="true">/blog/tutorial/Elasticsearch</guid>
        
        <category>Basics</category>
        
        
        <category>Tutorial</category>
        
      </item>
    
      <item>
        <title>Error Handling in Golang</title>
        <description>&lt;h3 id=&quot;what-is-error-handling-in-golang&quot;&gt;What is error handling in golang&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;Software will contain errors and scenarios that are not accounted for. Error handling is an important part of programming robust, reliable, and maintainable code.
Go offers an interesting approach to errors(a type in the language):
	Errors may be passed around functions and methods
Handling errors is part of creating robust, reliable, &amp;amp; trustworthy code
Calling a method or function that could fail, one of Go’s conventions is to return an error type as its final value

Generally no exception will be thrown within a function if something goes wrong it is up to the caller to decide what to do with the error
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;understanding-the-error-type&quot;&gt;UNDERSTANDING THE ERROR TYPE&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;In Go, an error is a value
The standard library declares the error interface as follows:

	type error interface {
    	Error() string

This features a single method called Error that returns a string
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;creating-error-object&quot;&gt;Creating error object&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;package main
import (
	&quot;errors&quot;
	&quot;fmt&quot;
)
func main() {
	err := errors.New(&quot;Something went wrong&quot;)
	if err != nil {
		fmt.Println(err)
	}
}

An error is created using the New method from the errors package
An if statement checks whether the error value was not nil
If the value is found to not be nil, it is printed
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;dont-panic&quot;&gt;DON’T PANIC&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;Panic - built-in function in Go - stops ordinary flow of control, begins panicking, halting the execution of the program
Bad idea to use this, as execution stops without any alternative
Below example demonstrates how panic is a hard stop
	package main
	import (
	    &quot;fmt&quot;
	)
	func main() {
	    fmt.Println(&quot;This is executed&quot;)
	    panic(&quot;Oh no. I can do no more. Goodbye.&quot;)
	    fmt.Println(&quot;This is not executed&quot;)
	}
&lt;/code&gt;&lt;/pre&gt;

</description>
        <pubDate>Mon, 13 Mar 2023 15:20:00 +0530</pubDate>
        <link>/blog/tutorial/ErrorHandlingInGolang</link>
        <guid isPermaLink="true">/blog/tutorial/ErrorHandlingInGolang</guid>
        
        <category>Error</category>
        
        
        <category>Tutorial</category>
        
      </item>
    
      <item>
        <title>Two Factor Authentication</title>
        <description>&lt;h3 id=&quot;2-factor-authentication&quot;&gt;2 Factor Authentication&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;2FA adds an extra layer of account protection by requiring two types of authentication. 
This can be something a user knows, like a password, and something the user has, like a phone(one-time passwords), inherence factors are things that the user is, typically a biometric characteristic such as a fingerprint or an iris pattern.
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;totp&quot;&gt;TOTP&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;Time-based One-Time Passwords(TOTP) is a common form of MFA specifically based on 2 Factor Authentication. TOTP Algorithm uses current time(device or server time) and shared key as input.
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;how-totp-works&quot;&gt;How TOTP works?&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;The inputs include a shared secret key and the system time.
Neither the inputs nor the calculation require internet connectivity to generate or verify a token. Diagram below shows this:
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;&lt;figure&gt;&lt;img src=&quot;/media/img/totp-2fa.jpg&quot; /&gt;&lt;figcaption&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;

&lt;pre&gt;&lt;code&gt;You can notice that the secret key and the time both at server end as well as user&apos;s end is same. And gives the passcode here for eg.&quot;343267&quot; is same at user&apos;s as well as server&apos;s end.
&lt;/code&gt;&lt;/pre&gt;
</description>
        <pubDate>Mon, 12 Oct 2020 15:20:00 +0530</pubDate>
        <link>/blog/tutorial/Two-Factor-Authentication</link>
        <guid isPermaLink="true">/blog/tutorial/Two-Factor-Authentication</guid>
        
        <category>2FA,</category>
        
        <category>TOTP,</category>
        
        <category>MFA</category>
        
        
        <category>Tutorial</category>
        
      </item>
    
      <item>
        <title>Fork vs Exec System call</title>
        <description>&lt;h3 id=&quot;what-is-fork-system-call&quot;&gt;What is Fork System Call?&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;When a new process is created by making an exact copy of itself it is fork. As this is the exact copy the environment remains same as that of parent process with the only difference is pid.
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;what-is-exec-system-call&quot;&gt;What is Exec System Call?&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;The exec system call replaces the current process image with the new process image. We can also say that after the fork process, the address space of the child process is overwritten with the new process data.
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;fork-and-exec-system-call-together&quot;&gt;Fork and Exec System call together?&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;The fork-and-exec switches the process with each other as in the following example:
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;&lt;figure&gt;&lt;img src=&quot;/media/img/forkandexec.jpg&quot; /&gt;&lt;figcaption&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;

&lt;pre&gt;&lt;code&gt;Have a look on the process they are same in case of fork and different incase of exec. Also the PID is different in case of fork and same incase of exec.
&lt;/code&gt;&lt;/pre&gt;

</description>
        <pubDate>Mon, 12 Oct 2020 15:20:00 +0530</pubDate>
        <link>/blog/tutorial/Fork-Vs-Exec-System-call</link>
        <guid isPermaLink="true">/blog/tutorial/Fork-Vs-Exec-System-call</guid>
        
        <category>fork</category>
        
        <category>system</category>
        
        <category>call,</category>
        
        <category>exec</category>
        
        <category>system</category>
        
        <category>call</category>
        
        
        <category>Tutorial</category>
        
      </item>
    
      <item>
        <title>Zombie vs Orphan process</title>
        <description>&lt;h3 id=&quot;what-is-process-and-how-it-is-reaped-when-dead&quot;&gt;What is process and how it is reaped when dead?&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;In computing, a process is an instance of a computer program that is being executed.
A process notifies its parent process once it has completed its execution and has exited. Then the parent process would remove the process from process table.
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;what-is-a-zombie-process&quot;&gt;What is a Zombie process?&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;Also known as “defunct” or “dead” process – In simple words, a Zombie process is one that is dead but it is present in the system’s process table. If the parent process is unable to read the process status from its child (the completed process), it won’t be able to remove the process from memory and thus the process being dead still continues to exist in the process table.

The following command can be used to find zombie processes:
$ ps aux | egrep &quot;Z|defunct&quot;

To remove Zombie process entry from process table, what can be done is to notify its parent process explicitly to remove the  entry. This can be done by sending a SIGCHLD signal to the parent process. 

The following command can be used to find the parent process ID (PID):
$ ps -o ppid= &amp;lt;child pid&amp;gt;

Once you have the Zombie’s parent process ID, you can use the following command to send a SIGCHLD signal to the parent process:

$ kill -s SIGCHLD &amp;lt;parent pid&amp;gt;

If this does not help t delete entry ofZombie process, you will have to kill. The following command can be used to kill its parent process:

$ kill -9 &amp;lt;parent pid&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;what-is-an-orphan-process&quot;&gt;What is an Orphan process?&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;An orphan process is a running process whose parent process has finished or terminated.A process can be intentionally orphaned just to keep it running.

Any orphaned process will be immediately adopted by the special init system process. This operation is called re-parenting and occurs automatically.

Even though technically the orphan process has the init process as its parent, it is still called an orphan process since the process that originally created it no longer exists.

Finding a Orphan Process:
It’s very easy to spot a Orphan process. Orphan process is a user process, which is having init (process id – 1) as parent. You can use this command in linux to find the Orphan processes.

$ ps -elf | head -1; ps -elf | awk &apos;{if ($5 == 1 &amp;amp;&amp;amp; $3 != &quot;root&quot;) {print $0}}&apos; | head
This will show you all the orphan processes running in your system. The output from this command confirms that they are Orphan processes, i.e. the process couldbe intentionally orphaned, so confirm from some other source also before killing them.

Killing a Orphan Process:
As orphaned processes waste server resources, so it’s not advised to have lots of orphan processes running into the system. To kill a orphan process is same as killing a normal process. gracefuly using -15 (term)

$ kill -15 &amp;lt;pid&amp;gt;
If that don’t work then simply use
$ kill -9 &amp;lt;pid&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
</description>
        <pubDate>Tue, 15 Sep 2020 14:40:00 +0530</pubDate>
        <link>/blog/tutorial/Zombie-Orphan-process</link>
        <guid isPermaLink="true">/blog/tutorial/Zombie-Orphan-process</guid>
        
        <category>Zombie</category>
        
        <category>process,Orphan</category>
        
        <category>process</category>
        
        
        <category>Tutorial</category>
        
      </item>
    
      <item>
        <title>Querying Dremio using Golang and ODBC driver</title>
        <description>&lt;pre&gt;&lt;code&gt;Dremio can be accessed via golang or other programming language. Here we are moving ahead by having two docker containers one of dremio and other of the database postgres.
To access it via code or isql you need to have Dremio odbc Driver and unixODBC driver installed as we are in Linux environment (For windos it is not required).

And that is it. Let follow the steps below.
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;prerequisites&quot;&gt;Prerequisites:&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;Docker installed
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;run-postgres-and-dremio-container&quot;&gt;Run postgres and dremio container&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;Postgres:
docker run --name some-postgres -p 5432:5432 -e POSTGRES_PASSWORD=mysecretpassword -d postgres

Dremio:
docker run -d -p 9047:9047 -p 31010:31010 -p 45678:45678 dremio/dremio-oss
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;configure-dremio-to-get-the-postgress-data&quot;&gt;Configure dremio to get the postgress data&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;* Go to http://127.0.0.1/9047 to access dremio
* Create account in dremio by providing the details, and do remember username and password.
* Go to → add source → select postgres → provide the required details
* To get postgres ip you can go inside the container and do cat /etc/hosts and you can find the ip
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;

&lt;figure&gt;
&lt;img src=&quot;/media/img/dremio-postgres1.png&quot; /&gt;
&lt;figcaption&gt;Login Details&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;/div&gt;

&lt;pre&gt;&lt;code&gt;Once connected follow the steps below.
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;install-dremio-odbc-driver&quot;&gt;Install Dremio odbc driver&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;$ wget https://download.dremio.com/odbc-driver/1.4.2.1003/dremio-odbc-1.4.2.1003-1.x86_64.rpm
$ sudo yum localinstall dremio-odbc-1.4.2.1003-1.x86_64.rpm
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;install-go&quot;&gt;Install go&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ wget https://dl.google.com/go/go1.13.linux-amd64.tar.gz
$ sudo tar -C /usr/local -xzf go1.13.linux-amd64.tar.gz
$ vi ~/.bash_profile
	export PATH=$PATH:/usr/local/go/bin
$ source ~/.bash_profile 
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;setting-up-odbcini-and-odbcinstini-file&quot;&gt;Setting up odbc.ini and odbcinst.ini file&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;Copy and Paste the below file to your odbc.ini file and odbcinst.ini respectively. You can find the location of ini file by the following command:

$ odbcinst -j

$ sudo vi /etc/odbc.ini
		[ODBC Data Sources]
		DREMIO=Dremio ODBC Driver 64-bit

		[DREMIO]
		Description=Dremio ODBC Driver (64-bit) DSN
		Driver=/opt/dremio-odbc/lib64/libdrillodbc_sb64.so
		ConnectionType=Direct
		HOST=192.168.1.4
		PORT=31010
		DATABASE=mydb
		ZKQuorum=
		ZKClusterID=
		AuthenticationType=Plain
		DelegationUID=
		AdvancedProperties=CastAnyToVarchar=true;HandshakeTimeout=5;QueryTimeout=180;TimestampTZDisplayTimezone=utc;NumberOfPrefetchBuffers=5;
		Catalog=DREMIO
		Schema=
		SSL=0
		DisableHostVerification=0
		DisableCertificateVerification=0
		TrustedCerts=/opt/dremio-odbc/lib64/cacerts.pem
		UseSystemTrustStore=0
		UseExactTLSProtocolVersion=0
		Min_TLS=
		TLSProtocol=

$ sudo vi /etc/odbcinst.ini
		[Dremio ODBC Driver 64-bit]
		Description=Dremio ODBC Driver(64-bit)
		Driver=/opt/dremio-odbc/lib64/libdrillodbc_sb64.so
		UsageCount=1
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;install-unixodbc-driver&quot;&gt;Install unixODBC driver&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;$ wget ftp://ftp.unixodbc.org/pub/unixODBC/unixODBC-2.3.7.tar.gz
$ gunzip unixODBC-2.3.7.tar.gz
$ tar xvf unixODBC*.tar
$ cd unixODBC-2.3.7/
$ ./configure  --prefix=/usr/local/unixODBC
$ make
$ sudo make install
$ export CGO_CFLAGS=&quot;-I/usr/local/unixODBC/include&quot;
$ export CGO_LDFLAGS=&quot;-L/usr/local/unixODBC/lib&quot;
$ go get github.com/alexbrainman/odbc
$ go run test.go
		package main
		import (
		_ &quot;github.com/alexbrainman/odbc&quot;
		&quot;database/sql&quot;
		&quot;log&quot;
		&quot;fmt&quot;
		)
		func main() {
		    var (
		        ID   int
		        NAME string
			   AGE int
			   ADDRESS string
			   SALARY float64
		    )
		    db, err := sql.Open(&quot;odbc&quot;,&quot;DSN=Dremio;UID=&amp;lt;dremio login username&amp;gt;;PWD=&amp;lt;dremio password&amp;gt;&quot;)
		    if err != nil {
		                log.Fatal(err)
		    }
		    rows, err := db.Query(&quot;SELECT * from test.company&quot;)
		    if err != nil {
		            log.Fatal(err)
		    }
		    if err != nil {
		        fmt.Println(&quot;Unable to Query :( &quot;, err)
		    }
		    defer rows.Close()
		    for rows.Next() {
		        err := rows.Scan(&amp;amp;ID,&amp;amp;NAME,&amp;amp;AGE,&amp;amp;ADDRESS,&amp;amp;SALARY )
		        if err != nil {
		            fmt.Println(&quot;Error when parsing :( &quot;, err)
		        }
		 
		        fmt.Println(ID, NAME, AGE, ADDRESS, SALARY)
		    }
		    err = rows.Err()
		    if err != nil {
		        fmt.Println(err)
		    }
			defer db.Close()
		}
&lt;/code&gt;&lt;/pre&gt;

</description>
        <pubDate>Sat, 27 Jun 2020 22:40:00 +0530</pubDate>
        <link>/blog/tutorial/dremio_odbc_golang</link>
        <guid isPermaLink="true">/blog/tutorial/dremio_odbc_golang</guid>
        
        <category>Dremio,</category>
        
        <category>Odbc</category>
        
        <category>,</category>
        
        <category>Golang,</category>
        
        <category>Postgres,</category>
        
        <category>Linux</category>
        
        
        <category>Tutorial</category>
        
      </item>
    
      <item>
        <title>CI/CD pipeline with Jenkins, docker and kubernetes</title>
        <description>&lt;h3 id=&quot;setup-jenkins-server&quot;&gt;Setup Jenkins server&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ mkdir jenkins
$ cd jenkins
$ vim Vagrantfile

Vagrant.configure(&quot;2&quot;) do |config|
  config.vm.box = &quot;bento/ubuntu-20.04&quot;
  config.vm.network &quot;forwarded_port&quot;, guest: 8080, host: 8081, host_ip: &quot;127.0.0.1&quot;
  config.vm.network &quot;private_network&quot;, ip: &quot;192.168.10.31&quot;

  config.vm.provision &quot;shell&quot;, inline: &amp;lt;&amp;lt;-SHELL
    sudo apt-get update
    sudo apt-get -y upgrade
    sudo apt install openjdk-11-jdk -y  
    wget -q -O - https://pkg.jenkins.io/debian-stable/jenkins.io.key | sudo apt-key add -
    echo &quot;deb https://pkg.jenkins.io/debian-stable binary/&quot; | sudo tee -a  /etc/apt/sources.list &amp;gt; /dev/null
    sudo apt-get update
    sudo apt-get install -y jenkins
  SHELL
end

$ vagrant up
$ vagrabt ssh
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;setup-repository-for-docker&quot;&gt;Setup Repository for docker&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ sudo apt-get update \
$ sudo apt-get install \
    apt-transport-https \
    ca-certificates \
    curl \
    gnupg-agent \
    software-properties-common


$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
$ sudo apt-key fingerprint 0EBFCD88

$ sudo add-apt-repository \
   &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \
   $(lsb_release -cs) \
   stable&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;install-docker-engine&quot;&gt;Install docker engine&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ sudo apt-get update
$ sudo apt-get install docker-ce docker-ce-cli containerd.io
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;updated-user-jenkins-to-run-docker&quot;&gt;updated user jenkins to run docker&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ sudo usermod -G docker jenkins	
$ sudo apt-get install acl

$ sudo setfacl -m user:jenkins:rw /var/run/docker.sock
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;setup-kubernetes&quot;&gt;Setup Kubernetes&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl

$ chmod +x ./kubectl
$ sudo mv ./kubectl /usr/local/bin/kubectl

$ cp -r ~/.kube /var/lib/jenkins/
$ sudo chown -R jenkins:jenkins /var/lib/jenkins/
$ vim /var/lib/jenkins/.kube/config

		apiVersion: v1
		clusters:
		- cluster:
		    certificate-authority: /var/lib/jenkins/.minikube/ca.crt
		    server: https://192.168.99.100:8443
		  name: minikube
		contexts:
		- context:
		    cluster: minikube
		    user: minikube
		  name: minikube
		current-context: minikube
		kind: Config
		preferences: {}
		users:
		- name: minikube
		  user:
		    client-certificate: /var/lib/jenkins/.minikube/profiles/minikube/client.crt
		    client-key: /var/lib/jenkins/.minikube/profiles/minikube/client.key


❖ make sure the permission of the following files should be as follows
	➢ client.crt 644, 
	➢ client.key 600, 
	➢ ca.crt 644


$ Adding docker hub credentials in jenkins
Credentials → System → Global Credentials → Add Credentials
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;

&lt;figure&gt;
&lt;img src=&quot;/media/img/credentials.png&quot; /&gt;
&lt;figcaption&gt;Login Details&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;/div&gt;

&lt;p&gt;Job → Pipeline → Poll SCM(after every push it will auto build the job)&lt;/p&gt;

&lt;div&gt;

&lt;figure&gt;
&lt;img src=&quot;/media/img/pollscm.png&quot; /&gt;
&lt;figcaption&gt;Poll Scm&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;/div&gt;

&lt;pre&gt;&lt;code&gt;❖ Add the groovy script
	➢ either via copy
	➢ or via SCM
❖ Apply → Save → Build
❖ Test on command line
	➢ kubectl get svc
	➢ https://192.168.10.30:&amp;lt;&amp;lt; port &amp;gt;&amp;gt;/hello/deepak
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;the-groovy-script&quot;&gt;The groovy script:&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;pipeline {
   agent any
   environment {
       dockerHub = &quot;docker.io&quot;
       docker_cred = &apos;dockerhub&apos;
   }
   stages {
		stage(&apos;SCM Checkout&apos;){
			steps{
				checkout([$class: &apos;GitSCM&apos;,branches: [[name: &apos;origin/master&apos;]],extensions: [[$class: &apos;WipeWorkspace&apos;]],userRemoteConfigs: [[url: &apos;https://github.com/agrawalpoonam/python-app.git&apos;]]  ])
			}
		}
		stage(&quot;Build Docker Image&quot;){
			steps{
				sh &apos;pwd&apos;
				sh &quot;docker build -t poonamag/test-app . --no-cache&quot;
			}
		}
		stage(&apos;Upload Image to DockerHub&apos;){
			steps{ 
	     	    withCredentials([
     		 	[$class: &apos;UsernamePasswordMultiBinding&apos;, credentialsId: docker_cred, usernameVariable: &apos;dockeruser&apos;, passwordVariable: &apos;dockerpass&apos;],
  				]){
					sh &quot;docker login -u ${dockeruser} -p ${dockerpass} ${dockerHub}&quot;
  				}
	    	  	sh &apos;docker push poonamag/test-app&apos;
	    	 }
	  	}
		stage(&quot;Running Docker Image&quot;){
			steps{
				sh &quot;kubectl get namespaces&quot;
				sh &quot;kubectl apply -f deployment.yaml&quot;
			}
		}
}
}
&lt;/code&gt;&lt;/pre&gt;

</description>
        <pubDate>Sun, 14 Jun 2020 22:30:00 +0530</pubDate>
        <link>/blog/tutorial/cicd_jenkins_dockr_kubernetes</link>
        <guid isPermaLink="true">/blog/tutorial/cicd_jenkins_dockr_kubernetes</guid>
        
        <category>Jenkins,</category>
        
        <category>Docker</category>
        
        <category>,</category>
        
        <category>Kubernetes</category>
        
        
        <category>Tutorial</category>
        
      </item>
    
  </channel>
</rss>
